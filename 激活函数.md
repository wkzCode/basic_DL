# 激活函数

## Sigmoid
$$sigmoid(x)=\frac{1}{1+e^{-x}}$$
$$sigmoid'(x)=sigmoid(x)(1-sigmoid(x))$$
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 反向传播梯度计算
def sigmoid_gradient(x):
    s = sigmoid(x)
    return s * (1 - s)
```

优点
* 输出概率解释性强

缺点
* 两端梯度趋近0，可能导致梯度消失
* 输出不是以0为中心，可能导致梯度向特定方向移动
* 指数计算，速度慢

## Tanh
$$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1$$

缺点
* 两端梯度趋近0，可能导致梯度消失
* 指数计算，速度慢


## ReLU
$$ReLU(x)=max(0,x)$$
优点
* 解决梯度消失问题
* 收敛快速
* 计算复杂度低

缺点
* 输出不是以0为中心
* Dead ReLU：输入为负时梯度为0，相应参数永远不会被更新

## Leaky ReLU
$$L\_ReLU(x)=max(\alpha x,x)$$

优点
* 解决Dead ReLU问题
* 计算复杂度低
* 收敛快

缺点
* $\alpha$需要人工经验赋值，一般为0.01
* 近似线性，在复杂分类中表现不好

## PReLU
将Leaky ReLU中的$\alpha$改为通过学习得到

## ELU
$$ELU(x)=\alpha (e^x-1) , x<0$$
优点
* 加快学习速度
* 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息

缺点
* 计算复杂度高

## SELU

## Swish
$$Swish(x)=x*sigmoid(x)$$
优点
* 平滑
* 非单调

缺点
* 计算成本略高

## Mish
$$Mish(x)=x*tanh(ln(1+e^x))$$
优点
* 平滑
* 非单调

缺点
* 计算成本相对较高

## softmax
$$softmax(x)=\frac{e^{x_i}}{\sum_i e^{x_i}}$$